<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2025-03-03T18:34:17+01:00</updated><id>/feed.xml</id><title type="html">SprintML Lab</title><subtitle>Website of the SprintML lab.
</subtitle><entry><title type="html">Captured by Captions: On Memorization and its Mitigation in Multi-Modal Models</title><link href="/2025/03/03/CLIPMem.html" rel="alternate" type="text/html" title="Captured by Captions: On Memorization and its Mitigation in Multi-Modal Models" /><published>2025-03-03T00:00:00+01:00</published><updated>2025-03-03T00:00:00+01:00</updated><id>/2025/03/03/CLIPMem</id><content type="html" xml:base="/2025/03/03/CLIPMem.html"><![CDATA[<center>
  <a href="https://wenhaowang1995.github.io/">Wenhao Wang<sup>1</sup></a>,
  <a href="https://adam-dziedzic.com/">Adam Dziedzic<sup>1</sup></a>,
  <a href="https://oue.gatech.edu/node/3087">Grace Kim<sup>2</sup></a>,
  <a href="https://cispa.de/en/people/backes">Michael Backes<sup>1</sup></a>,
  <a href="https://franziska-boenisch.de/">Franziska Boenisch<sup>1</sup></a>.
  <br /><br />
  <sup>1</sup>CISPA Helmholtz Center for Information Security<br />
  <sup>2</sup>Georgia Institute of Technology<br />
    <br /><br />
</center>

<p><strong>Multi-modal models</strong> like <strong>CLIP</strong> are widely used for their state-of-the-art performance on a variety of downstream tasks. But while memorization is well-studied in uni-modal models, its role in multi-modal models is underexplored.</p>

<p>We focus on CLIP, as a representative multi-modal model, and analyze how much it memorizes its training data. We design a new CLIPMem metric to analyze the memorization behavior of CLIP. We observe that the memorization patterns in CLIP fall in between the ones found in <strong>supervised learning</strong> and <strong>self-supervised learning</strong> models. We also find that the <strong>text encoder</strong> in CLIP contributes more memorization than <strong>image encoder</strong>. Using these insights, we propose <strong>mitigation strategies</strong> that reduce memorization while <strong>simultaneously improving utility</strong> ‚Äì which has not been observed in traditional learning paradigms.</p>

<h2 id="how-to-measure-memorization-in-clip">How to measure memorization in CLIP?</h2>

<p>CLIP trains an image encoder and a text encoder to map images and captions into a shared latent space. Using <strong>contrastive learning</strong>, it pulls together matching image-text pairs while pushing apart non-matching pairs. Specifically, it maximizes the similarity score between correct pairs while minimizing it for incorrect pairs.</p>

<p>To measure memorization, we introduce <strong>CLIPMem</strong>, a new metric that leverages CLIP‚Äôs contrastive learning objective. CLIPMem quantifies memorization by measuring similarity scores across two models:</p>

<ul>
  <li>A model trained on the <strong>full dataset</strong>.</li>
  <li>A <strong>reference model</strong> trained on the same dataset but with <strong>one image-text pair removed</strong>.</li>
</ul>

<p>If the model aligns the removed pair much more strongly than the reference model, we consider it memorized. Our results show that <strong>mislabeled or atypical samples have the highest memorization</strong> (Figure 1).</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/clipmem/fig1.png" width="80%" /></p>
</div>

<h2 id="is-the-memorization-behavior-of-clip-more-similar-to-supervised-or-self-supervised-learning">Is the memorization behavior of CLIP more similar to Supervised or Self-Supervised Learning?</h2>

<p>In uni-modal models, both <strong>supervised learning (SL) and self-supervised learning (SSL)</strong> rely on memorization to improve generalization, but they have different memorization behaviors. It‚Äôs unclear how these memorization behaviors extend to CLIP, because <strong>CLIP entails elements from both</strong> ‚Äì it gets supervisory signals through captions <strong>(like SL)</strong> but it also learns through the contrastive objective <strong>(like SSL)</strong>.</p>

<p>Our analysis shows that CLIP‚Äôs memorization behavior falls between SL and SSL:</p>

<p>‚úÖ Like <strong>SL</strong>, CLIP memorizes mislabeled or imprecise samples.</p>

<p>‚úÖ Like <strong>SSL</strong>, CLIP memorizes atypical samples.</p>

<p>‚úÖ Neuron-level analysis shows that earlier layers behave like <strong>SL</strong>, grouping similar data points together, while later layers behave like <strong>SSL</strong>, memorizing individual data points (Figure 4).</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/clipmem/fig2.png" width="50%" /></p>
</div>

<p>Overall, CLIP doesn‚Äôt fully align with the memorization behavior of either SL or SSL, but instead falls somewhere in between.</p>

<h2 id="is-the-text-encoder-or-image-encoder-more-responsible-for-memorization">Is the text encoder or image encoder more responsible for memorization?</h2>

<p>CLIP has both the image encoder and the text encoder. But does one contribute more to memorization, or do they do so equally? To figure it out, we applied different augmentation strategies to see their effect on memorization.</p>

<p>Table 1 summarizes the findings:</p>

<p>‚úÖ <strong>Text augmentations</strong> (multiple captions per image) were more effective at reducing memorization than <strong>image augmentations</strong> (multiple images per caption) while also improving performance.</p>

<p>‚úÖ <strong>Applying both text and image augmentations</strong> strikes the best balance ‚Äì leading to the most reduction in memorization while also improving performance.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/clipmem/fig3.png" width="50%" /></p>
</div>

<p>Overall, these results show that <strong>the text encoder contributes more to memorization than the image encoder</strong>. This means that if we want to mitigate memorization in CLIP, we should <strong>focus more on the text domain</strong> ‚Äì like increasing caption diversity or adding noise to text embeddings (detailed in the next section).</p>

<h2 id="so-how-do-we-mitigate-memorization-in-clip">So how do we mitigate memorization in CLIP?</h2>

<p>Based on our insights, we propose effective mitigation strategies that reduce memorization. A surprising finding from our study is that <strong>reducing memorization can actually improve downstream generalization</strong>! This is not something that‚Äôs typically observed in traditional learning paradigms, where reducing memorization often results in a decrease in utility.</p>

<h3 id="mitigation-strategies">Mitigation strategies:</h3>

<p>1Ô∏è‚É£ Using multiple captions per image</p>

<p>Our result shows that <strong>increasing the number of captions used during training</strong> lowers memorization and improves downstream generalization (Figure 5a).</p>

<p>2Ô∏è‚É£ Noising text embedings</p>

<p>To avoid any inherent distribution shifts from augmentations, we propose to perform the ‚Äúaugmentations‚Äù <strong>directly in the embedding space</strong>. Adding small amounts of Gaussian noise to text embeddings during training reduced memorization while improving downstream generalization (Figure 5b).</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/clipmem/fig4.png" width="80%" /></p>
</div>

<p>3Ô∏è‚É£ Removing Memorized Samples</p>

<p>By <strong>identifying the most memorized samples</strong> via CLIPMem and <strong>removing them</strong>, we actually improved generalization (Figure 6).</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/clipmem/fig5.png" width="80%" /></p>
</div>

<h2 id="why-is-this-important">Why is this important?</h2>

<p>CLIP models are usually trained on <strong>large, uncurated datasets sourced from the internet</strong>, with no guarantees regarding the correctness of the image-text pairs. By using <strong>CLIPMem</strong>, we can identify and remove these problematic training examples, making CLIP both <strong>more private and more generalizable</strong>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Let‚Äôs summarize our findings:</p>

<p>1Ô∏è‚É£ <strong>CLIPMem</strong> is our new metric that measures memorization in CLIP.</p>

<p>2Ô∏è‚É£ The <strong>memorization behavior</strong> of CLIP falls between supervised learning and self-supervised learning.</p>

<p>3Ô∏è‚É£ The <strong>text encoder is more responsible</strong> for memorization than the image encoder.</p>

<p>4Ô∏è‚É£ <strong>Effective mitigation strategies</strong> include using multiple captions per image, noising text embeddings, and removing memorized samples.</p>

<p>5Ô∏è‚É£ Unlike traditional learning paradigms, <strong>reducing memorization in CLIP can actually performance</strong>.</p>

<hr />

<p><strong>üëâ Read our:</strong> <a href="https://openreview.net/pdf?id=5V0f8igznO" target="_blank"> full research paper accepted at ICLR 2025.</a></p>

<h2 id="bibtex">Bibtex</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{wang2025captured,
title={Captured by Captions: On Memorization and its Mitigation in {CLIP} Models},
author={Wenhao Wang and Adam Dziedzic and Grace C. Kim and Michael Backes and Franziska Boenisch},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=5V0f8igznO}
}
</code></pre></div></div>]]></content><author><name></name></author><category term="memorization" /><category term="multi-modal" /><category term="CLIP" /><category term="SSL" /><category term="SL" /><summary type="html"><![CDATA[Wenhao Wang1, Adam Dziedzic1, Grace Kim2, Michael Backes1, Franziska Boenisch1. 1CISPA Helmholtz Center for Information Security 2Georgia Institute of Technology]]></summary></entry><entry><title type="html">Image AutoRegressive Models Leak More Training Data Than Diffusion Models</title><link href="/2025/02/04/iars-privacy.html" rel="alternate" type="text/html" title="Image AutoRegressive Models Leak More Training Data Than Diffusion Models" /><published>2025-02-04T00:00:00+01:00</published><updated>2025-02-04T00:00:00+01:00</updated><id>/2025/02/04/iars-privacy</id><content type="html" xml:base="/2025/02/04/iars-privacy.html"><![CDATA[<center>
  <a href="http://antonikowalczuk.com">Antoni Kowalczuk*<sup>1</sup></a>,
  <a href="https://scholar.google.com/citations?user=iG319iwAAAAJ&amp;hl=pl&amp;oi=ao">Jan Dubi≈Ñski*<sup>2,3</sup></a>,
  <a href="https://franziska-boenisch.de/">Franziska Boenisch<sup>1</sup></a>,
  <a href="https://adam-dziedzic.com/">Adam Dziedzic<sup>1</sup></a>
  <br /><br />
  <sup>1</sup>CISPA Helmholtz Center for Information Security<br />
  <sup>2</sup>Warsaw University of Technology<br />
  <sup>3</sup>IDEAS NCBR<br />
  <em>*Indicates Equal Contribution</em>
    <br /><br />
</center>

<p>Image AutoRegressive models (IARs) have recently emerged as a powerful alternative to diffusion models (DMs), surpassing them in image generation quality, speed, and scalability. Yet, despite their advantages, the privacy risks of IARs remain completly unexplored. When trained on sensitive or copyrighted data, these models may unintentionally expose training samples, creating major security and ethical concerns.</p>

<p>In this blog post, which is based on our latest research paper, we investigate privacy vulnerabilities in IARs, showing that they exhibit significantly higher privacy risks compared to DMs. We assess IARs‚Äô privacy risks from the three perspectives of <strong>membership inference</strong>, <strong>dataset inference</strong>, and <strong>memorization</strong>, and find that IARs reveal substantially more information about their training data than DMs. Along the way, we also discuss ways to mitigate these risks. Let‚Äôs dive in!</p>

<h2 id="image-autoregressive-models-a-faster-but-riskier-approach"><strong>Image AutoRegressive Models: A Faster but Riskier Approach</strong></h2>

<p>IARs work like language models such as GPT, but instead of words, they generate images using tokens. Just as language tokens represent words or parts of words, image tokens represent some part of an image. This autoregressive approach allows them to scale effectively, resulting in <strong>higher quality and faster image generation</strong> compared to DMs. However, as we will show, these benefits come with severe <strong>privacy trade-offs</strong>.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/iars/pareto1.png" width="50%" /></p>
</div>

<p>Our evaluations on privacy attacks show that IARs are much more likely to <strong>reveal whether a specific image was used in its training</strong> and even <strong>expose parts of the training data</strong>. In fact, the largest IAR models leak entire training images verbatim!</p>

<h2 id="how-to-assess-privacy-risks-in-iars"><strong>How to Assess Privacy Risks in IARs</strong></h2>

<p>To uncover the privacy risks of IARs, we use three attack techniques‚Äîrefining existing methods and creating new ones from scratch‚Äîto measure how much these models expose their training data:</p>

<ol>
  <li><strong>Membership Inference Attacks (MIA):</strong> Determines whether a specific image was used in training.</li>
  <li><strong>Dataset Inference (DI):</strong> Detects whether an entire dataset was part of the model‚Äôs training set.</li>
  <li><strong>Data Extraction:</strong> Extracts full training images from IARs.</li>
</ol>

<p>These attacks help us quantify how much private data IARs unintentionally reveal compared to DMs.</p>

<hr />

<h3 id="membership-inference-iars-vs-dms"><strong>Membership Inference: IARs vs DMs</strong></h3>

<p>The goal of MIAs is to infer whether a specific image was part of the training dataset of a given model. In our study, we first show how MIAs developed against LLMs can be applied to IARs. Then we design <strong>a novel MIA approach optimized for IARs</strong> by leveraging their classifier-free guidance mechanism.</p>

<p>Our findings are striking: <strong>large IARs leak training membership information at much higher rates than diffusion models</strong>. The following figure shows the MIA success rates at a strict false positive rate (FPR) of 1%.</p>

<!-- ![pareto_teaser-min](https://hackmd.io/_uploads/HkKUffltkg.png) -->

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/iars/membershipinference.png" width="50%" /></p>
</div>

<p>As seen above, <strong>VAR-<em>d</em>30</strong>, one of the largest IARs, is highly vulnerable to MIAs, with an <strong>86.38% success rate</strong> in detecting training images. This is almost <strong>20√ó higher than the best MIA attack on the least private diffusion model</strong>, showing that IARs are inherently more prone to privacy attacks.</p>

<!-- #### **Table 1: Membership Inference Performance (TPR@FPR=1%)**
| Model      | Type | True Positive Rate @ 1% FPR |
|-----------|------|--------------------------|
| VAR-d16   | IAR  | 2.16%  |
| VAR-d30   | IAR  | **86.38%**  |
| RAR-XXL   | IAR  | 49.80%  |
| MAR-H     | IAR  | 3.40%  |
| MDTv2-XL  | DM   | **4.91%**  |

As seen above, **VAR-d30**, one of the largest IARs, is highly vulnerable to MIAs, with an **86.38% success rate** in detecting training images. This is almost **20√ó higher than the best MIA attack on diffusion models**, showing that IARs are inherently more prone to privacy attacks.
 -->

<hr />

<h3 id="dataset-inference-detecting-if-an-iar-was-trained-on-a-dataset"><strong>Dataset Inference: Detecting If an IAR Was Trained on a Dataset</strong></h3>

<p>Dataset Inference (DI) extends MIAs by resolving whether an entire dataset was used in training. This is particularly important for detecting unauthorized usage of private or copyrighted datasets.</p>

<p>We find that <strong>IARs require significantly fewer samples for successful DI compared to DMs</strong>, meaning they reveal more statistical information about their training datasets. The figure below summarizes the number of samples required to confirm dataset membership.</p>

<!-- #### **Table 2: Dataset Inference Success Rates**
| Model      | Type | Samples Required for DI |
|-----------|------|----------------------|
| VAR-d16   | IAR  | 200  |
| VAR-d30   | IAR  | **6**  |
| MAR-H     | IAR  | 300  |
| RAR-XXL   | IAR  | 8  |
| MDTv2-XL  | DM   | **200**  | -->

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/iars/datasetinference.jpg" width="50%" /></p>
</div>

<p>For <strong>VAR-<em>d</em>30</strong>, <strong>only 6 samples</strong> are needed to determine whether a dataset was used for training. This is an <strong>order-of-magnitude lower</strong> than the number required for diffusion models, indicating a much higher privacy leakage.</p>

<hr />

<h3 id="extracting-training-images-from-iars"><strong>Extracting Training Images from IARs</strong></h3>

<p>Beyond membership and dataset inference, we also explored whether <strong>IARs memorize and reproduce entire training images</strong>. We developed a <strong>data extraction attack</strong> that generates high-fidelity reconstructions of training images by prompting the model with a partial input.</p>

<p>We successfully extracted up to <strong>698</strong> verbatim training images from <strong>VAR-<em>d</em>30</strong>, proving that large IARs memorize and regurgitate data at an alarming rate, making them vulnerable to <strong>copyright infringement, privacy violations, and dataset exposure</strong>.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/iars/memorization.png" width="50%" /></p>
</div>

<p>The highly successful data extraction from IARs raises major privacy concerns, especially for commercial AI models trained on proprietary datasets.</p>

<h3 id="privacy-vs-performance-how-to-mitigate-privacy-risks-in-iars"><strong>Privacy vs. Performance: How to Mitigate Privacy Risks in IARs</strong></h3>

<p>Our findings reveal a clear <strong>privacy-utility trade-off</strong>. While IARs outperform Diffusion Models (DMs) in <strong>speed and image quality</strong>, they suffer from significantly <strong>higher privacy leakage</strong>. This presents a crucial question: can we achieve strong performance <strong>without sacrificing privacy?</strong></p>

<p>Given the severe privacy vulnerabilities of IARs, we explored potential <strong>mitigation strategies</strong>. Inspired by Differential Privacy (DP) techniques and prior research on language models, we experimented with the following defenses:</p>

<h4 id="1-adding-noise-to-logits"><strong>1. Adding Noise to Logits</strong></h4>

<p>For models like <strong>VAR</strong> and <strong>RAR</strong>, we attempted to <strong>add small amounts of noise</strong> to their logits to obscure membership signals. While this reduced MIA success rates slightly, it also <strong>deteriorated image quality</strong>, making it an impractical solution.</p>

<h4 id="2-noising-continuous-tokens-in-mar"><strong>2. Noising Continuous Tokens in MAR</strong></h4>

<p>For <strong>MAR</strong>, which operates with continuous tokens, we <strong>perturbed token values</strong> post-sampling. Interestingly, this defense was <strong>more effective than logit noise</strong> and caused only a <strong>minor drop in performance</strong>.</p>

<h4 id="3-leveraging-diffusion-techniques"><strong>3. Leveraging Diffusion Techniques</strong></h4>

<p>Our results indicate that <strong>MAR is inherently more private than VAR and RAR</strong>, likely because it incorporates <strong>diffusion-based techniques</strong>. This suggests that <strong>hybrid IAR-DM approaches</strong> could provide a good balance between <strong>performance and privacy</strong>.</p>

<hr />

<h3 id="final-thoughts-are-iars-too-risky-for-deployment"><strong>Final Thoughts: Are IARs Too Risky for Deployment?</strong></h3>

<p>Our study provides the first <strong>comprehensive privacy analysis</strong> of <strong>Image AutoRegressive Models (IARs)</strong>. The results are both <strong>impressive and alarming</strong>:</p>

<p>‚úÖ <strong>IARs are faster and more accurate</strong> than DMs.<br />
üö® <strong>IARs leak substantially more private data</strong> than DMs.</p>

<p>This raises a key concern: <strong>Can IARs be deployed safely?</strong></p>

<ul>
  <li>For <strong>non-sensitive applications</strong>, IARs offer a <strong>powerful and efficient alternative</strong> to DMs.</li>
  <li>However, for <strong>privacy-sensitive domains</strong> (e.g., medical imaging, biometric data, proprietary datasets), IARs <strong>pose severe risks</strong> unless stronger safeguards are in place.</li>
</ul>

<h3 id="conclusions"><strong>Conclusions</strong></h3>

<p>Let‚Äôs summarize our findings:</p>

<p>1Ô∏è‚É£ <strong>IARs have much higher privacy leakage than DMs</strong>, making them a riskier choice for privacy-sensitive applications.<br />
2Ô∏è‚É£ <strong>The larger the IAR, the more it memorizes and leaks training data.</strong><br />
3Ô∏è‚É£ <strong>Diffusion-based elements (like those in MAR) reduce privacy leakage</strong>, suggesting a potential direction for <strong>more private generative models</strong>. A promising approach is to synergize diffusion-based and autoregressive techqniques to provide high performance and lower privacy risk.<br />
4Ô∏è‚É£ <strong>Existing defenses are insufficient</strong>, and new privacy-preserving techniques <strong>must be developed before IARs can be safely deployed in sensitive areas</strong>.</p>

<h3 id="where-do-we-go-from-here"><strong>Where Do We Go from Here?</strong></h3>

<p>As <strong>IARs continue to evolve</strong>, privacy safeguards <strong>must</strong> become a core focus. Our research shows that <strong>adopting techniques from Diffusion Models</strong> and <strong>differential privacy methods</strong> could help <strong>mitigate privacy risks</strong>.</p>

<p>If you‚Äôd like to learn more, check out our full <strong>arXiv paper</strong>, where we dive deeper into <strong>membership inference attacks, dataset inference, and memorization risks</strong> in IARs.</p>

<p>üëâ <strong>Read the full paper: <a href="https://arxiv.org/abs/2502.02514">Privacy Attacks on Image AutoRegressive Models</a></strong></p>

<h2 id="bibtex">Bibtex</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{kowalczuk2025privacyattacksimageautoregressive,
      title={Privacy Attacks on Image AutoRegressive Models},
      author={Antoni Kowalczuk and Jan Dubi≈Ñski and Franziska Boenisch and Adam Dziedzic},
      year={2025},
      eprint={2502.02514},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.02514},
}
</code></pre></div></div>]]></content><author><name></name></author><category term="privacy" /><category term="image" /><category term="generative" /><category term="models" /><category term="diffusion" /><category term="models" /><category term="vision" /><category term="autoregressive" /><category term="models" /><summary type="html"><![CDATA[Antoni Kowalczuk*1, Jan Dubi≈Ñski*2,3, Franziska Boenisch1, Adam Dziedzic1 1CISPA Helmholtz Center for Information Security 2Warsaw University of Technology 3IDEAS NCBR *Indicates Equal Contribution]]></summary></entry><entry><title type="html">Private Adaptations of Open LLMs Outperform their Closed Alternatives</title><link href="/2024/12/10/open-llms.html" rel="alternate" type="text/html" title="Private Adaptations of Open LLMs Outperform their Closed Alternatives" /><published>2024-12-10T00:00:00+01:00</published><updated>2024-12-10T00:00:00+01:00</updated><id>/2024/12/10/open-llms</id><content type="html" xml:base="/2024/12/10/open-llms.html"><![CDATA[<p><em>by Adam Dziedzic and Franziska Boenisch</em></p>

<p>Nowadays, Large Language Models (LLMs) perform a plethora of language tasks. OpenAI exposes its GPT4 model to perform tasks, such as text generation, translation, dialog summarization, code generation, and many others. While <em>closed LLMs</em>, like GPT4, are exposed via public APIs or web interfaces, for <em>open LLMs</em>, like Llama, their parameters (i.e., their weights) are directly released and allow us to simply download them and and run the model locally. Therefore, we sometimes call these models ‚Äú<em>open-weight LLMs</em>‚Äù. Both types of models, open and closed ones, even though they have strong zero-shot performance, they still require <em>adaptations</em>, such as prompting or fine-tuning to perform well on specialized downstream tasks. Given that downstream tasks often rely on sensitive data, we need to ensure this data‚Äôs privacy when adapting LLMs.</p>

<p>In this blog post which is based on our latest <a href="https://openreview.net/forum?id=Jf40H5pRW0">NeurIPS 2024 paper</a>, <strong>we compare private adaptations for open vs. closed LLMs on multiple axes and find that by adapting open LLMs instead of closed ones, we can preserve more privacy and obtain higher performance at lower cost.</strong> On the way, we also design novel private prompting methods for generative tasks. Let‚Äôs explore how we do that.</p>

<h3 id="open-llms-have-comparable-performance-to-closed-llms">Open LLMs have comparable performance to Closed LLMs</h3>

<p>The most recent results from standard benchmarks (such as MMLU that measures knowledge acquired by LLMs during pretraining) show that <strong>open-weight LLMs such as Llama 3.1 405 B closed the gap in performance with closed-source LLMs for the first time</strong>.
This is a great starting point for our research.</p>

<h3 id="how-to-adapt-your-llm">How to Adapt your LLM</h3>

<p>As we mentioned above, LLMs perform well on <em>general</em> understanding tasks, however, they do not perform well enough on <em>specialized</em> downstream tasks. For example, for the DBpedia task, that aims to classify Wikipedia articles, we observe that after adapting the LLM to the task, we can boost the performance by more than 40%. This motivates the why it is important to adapt the models.
Additionally, behind the scenes, the training of entire large LLMs from scratch is a costly venture. Hence, adaptations are a computationally reasonable alternative.
Let‚Äôs dive into how we can adapt LLMs:</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/openLLMs/adaptations.png" width="50%" /></p>
  <p style="margin-top:10px;">Figure 1: The different ways of adapting LLMs.</p>
</div>

<p>One of the most popular ways to adapt LLMs is through prompting (denoted as 1. Input Prompt in Figure 1). The input prompts can be <em>discrete</em> where you prepend additional natural language text to your standard input. On the other hand, <em>soft prompts</em> are a learnable set of parameters prepended to the input embeddings. <em>Prefix tuning</em> is similar to the soft prompt but apart from being prepended to the input, it can also be prepended to every attention layer.</p>

<p>The second approach to adapt LLMs is through inner fine-tuning (the 2nd adaptation in Figure 1). You can do it either through full fine-tuning, where you adjust all the parameters of your LLM, or using a low-rank adaptation, abbreviated as LoRA, where a small set of additional parameters are added to many layers inside an LLM. Finally, we can fine-tune a couple of the last layers or even add an additional layer or more layers on top of an LLM (the 3-rd type of adaptation in Figure 1).</p>

<p><strong>Important!!!</strong> If you want to adapt a closed LLM such as GPT4, due to the API restrictions, you can only use discrete prompts or last-layer fine-tuning. These adaptations are less performant than gradient-based adaptations that we can use on open LLMs.</p>

<h3 id="more-privacy-is-leaked-through-adaptations-of-closed-vs-open-llms">More privacy is leaked through adaptations of closed vs open LLMs</h3>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/openLLMs/setup.png" width="50%" /></p>
  <p style="margin-top:10px;">Figure 2: How privacy can leak from closed LLMs.</p>
</div>

<p>When we want to reason about privacy leakage from LLM adaptations, it helps to understand the stakeholders that are involved in practice.
We identified the data curator, e.g., a company that holds their customers‚Äô private data. The company wants to adapt an LLM on this private data to solve a task for (potentially new) customers, i.e., querying parties. In case of a closed LLM, there is also an LLM provider, i.e., a company like OpenAI that deploys the models, such as GPT4.</p>

<p>With these stakeholders defined, let‚Äôs now look into the difference in privacy leakage between open and closed LLMs.</p>

<p><strong>Open LLMs.</strong> The data curator can locally adapt an open LLM on their private data so that it can cater to a specific downstream task. Then, a customer can query the adapted open LLM. In this case, the customer sends the query directly to the party hosting the LLM and no third parties are involved in the process. 
The only privacy concern arises since the querying party might be malicious. In this case, the private information from the data curator can leak to the querying party through the returned answers of the prompted LLM.</p>

<p><strong>Closed LLMs.</strong> Let us also consider the case when the data curator does not have any open LLM on-premise and would like to use a closed LLM from a model provider, for example, GPT4 exposed by the API from OpenAI. First, the closed LLM must be adapted to the private data. For the closed LLMs, this can be done by discrete prompts. 
This setup induces multiple axes of possible privacy leakage:
For adaptation, the private data has to be directly released to the LLM provider (case 1 in Figure 2). Additionally, the private queries from a customer must be routed through the LLM provider. Thus, also the private queries from the customers leak to the LLM providers (case 2). Finally, in the same way as with open LLMs, the answers can leak information contained in the private data to the querying party (case 3).</p>

<p>Overall, this conceptual analysis shows that the privacy leakage is much higher when adapting closed LLMs vs open LLMs. While for open LLMs, only answers can leak some private information, with closed LLMs, the private data and queries leak to the provider of the closed LLM.</p>

<p>Next, we will investigate how to prevent privacy leakage.</p>

<h3 id="private-llm-adaptations-for-text-generation">Private LLM Adaptations for text generation</h3>

<p>To obtain private LLM adaptations, we build on the mathematical framework of <a href="https://www.comp.nus.edu.sg/~tankl/cs5322/readings/dwork.pdf">Differential Privacy (DP)</a>. For LLM adaptations, DP formalizes the intuition that adaptations learned on two neighboring datasets, i.e., datasets that differ in only one data point, will induce roughly the same behavior of the adapted LLM.
The <em>roughly</em> is expressed in a privacy budget parameter Œµ.</p>

<p>Within all existing DP prompting methods, we identified that there is a lack of support for text generation tasks (e.g., dialog summarization). Instead, most prior work on private prompting focuses on <em>classification</em> tasks. This is a big gap since LLMs are generative models that solve inherently much more complex tasks than classification.</p>

<p>To understand our methods on private prompts for generative tasks, it might be helpful if you first check out our other blog post <a href="https://sprintml.com/2024/04/27/private-prompts.html">here</a>, where we introduced the first private prompts for LLMs.
Here, we only give you a short intuition on how our generation methods work. If you are interested in the details, feel free to check out our paper.</p>

<p>To enable private text generation with <strong>discrete prompts</strong>, we build on the differentially private PATE algorithm and introduce a privacy-preserving knowledge transfer from an ensemble of teacher prompts to a student prompt. The resulting method is called PromptDPSGDGen.
For methods like <strong>soft prompts</strong> or <strong>prefix</strong>, we adjust the differential private stochastic gradient descent (DPSGD) algorithm.
This method for text generation, called PromptDPSGDGen obtains the input gradients from the LLM and performs DPSGD to update the soft prompt parameters while keeping the underlying LLM frozen.</p>

<h3 id="private-llm-adaptations-on-open-llms-outperform-their-closed-alternatives">Private LLM Adaptations on open LLMs outperform their closed alternatives</h3>

<p>We carried out an in-depth comparison of the adaptations of open vs closed LLMs considering 1) privacy protection, 2) performance, and 3) cost. The <em>privacy protection</em> is assessed in terms of the leakage of private data either to the LLM provider or the querying party (a user of the adapted LLM), as well as the leakage of the users‚Äô queries to the LLM provider. The <em>performance</em> is measured in terms of accuracy for classification tasks and scores like Rouge or BLEU for text generation tasks at a given privacy budget Œµ. Finally, the <em>cost</em> is the amount of money in dollars needed to adapt a given LLM with privacy.</p>

<p>We analyzed four recent methods to adapt closed LLMs, including our PromptPATEGen. All of them were designed for closed LLMs to prevent the leakage of private data to the querying party. All of them fulfill this goal. However, they leak private data and queries to the LLM provider. The only method that does not leak private data to the LLM provider is <a href="https:
//openreview.net/forum?id=Ifz3IgsEPX">DP-OPT</a>. But DP-OPT requires the data curator to use an additional open LLM on-premise. 
For open LLMs, we compare private fine-tuning, soft prompts, including our PromptDPSGDGen, and private LoRA.
When applied to an open local model, none of them leaks private data and queries to an external LLM provider.</p>

<p>Overall, we find that the <strong>adaptations of open LLMs offer higher privacy protection and higher performance at lower cost</strong>. On the other hand, the prompt-based adaptations for closed LLMs provide lower privacy protection and lower performance at a higher cost compared to their open counterparts. We further analyze the privacy-utility trade-off for classification and generation tasks across different privacy budgets. 
Let‚Äôs look into some numbers: For Table 1 below, we adapted open and closed LLMs on dialog summarization (SAMSum dataset). 
The metrics used to report performance are Rouge scores. Rouge-1 assesses how many unigrams in the generated text agree with the expected LLM output. Rouge-2 is similar but uses bi-grams. Rouge-L refers to the similarity of the longest common subsequence between prediction and target.
In terms of cost, for <em>closed</em> LLMs, we aggregate the costs incurred by the API over adaptation and querying. 
For <em>open</em> LLMs, we estimate the costs through costs that would be incurred by adapting and querying the LLM on cloud hardware. The cost is, hence, dependent on the time it takes to adapt and query on the cloud hardware.
In both cases, open and closed model adaptations, for comparability, we assume 10k queries.</p>

<p>In the following table, we present some insights on adaptation for closed LLMs (DP-ICL and PromptPATEGen) and open LLMs (PromptDPSGDGen and Private LoRA). For more results on all methods and various LLMs, check our paper.</p>

<div style="display: table; margin: 0 auto;">

  <table>
    <thead>
      <tr>
        <th>Adaptation</th>
        <th>LLM</th>
        <th>Rouge-1</th>
        <th>Rouge-2</th>
        <th>Rouge-L</th>
        <th>Cost ($)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>DP-ICL</td>
        <td>GPT4-Turbo</td>
        <td>41.8</td>
        <td>17.3</td>
        <td>33.4</td>
        <td>3419</td>
      </tr>
      <tr>
        <td>PromptPATEGen</td>
        <td>Open Llama 13B</td>
        <td>43.4</td>
        <td>19.7</td>
        <td>34.2</td>
        <td>19.43</td>
      </tr>
      <tr>
        <td>PromptDPSGDGen</td>
        <td>BART Large</td>
        <td>46.1</td>
        <td>21.3</td>
        <td>37.4</td>
        <td>2.13</td>
      </tr>
      <tr>
        <td>Private LoRA</td>
        <td>BART Large</td>
        <td>48.8</td>
        <td>23.5</td>
        <td>39.1</td>
        <td>3.59</td>
      </tr>
      <tr>
        <td>Private LoRA</td>
        <td>Mixtral 8x7B</td>
        <td>52.8</td>
        <td>29.6</td>
        <td>44.7</td>
        <td>67.95</td>
      </tr>
    </tbody>
  </table>

  <p style="margin-top:10px;">Table 1: Comparison of private open vs closed LLM adaptations on SAMSum at a privacy budget of Œµ=8.</p>
</div>

<p>What is striking is that the adaptations for open LLMs achieve higher performance, expressed by higher Rouge scores, on significantly smaller models (BART with 355M parameters vs the significantly larger GPT4-Turbo and Open Llama with 13B parameters). This performance is achieved at a fraction of costs.
If we are willing to spend slightly more money and deploy a larger open LLM, such as Mixtral, the performance gain from the adaptation of the open LLM is even more significant than for the closed adaptations.</p>

<h3 id="conclusions">Conclusions</h3>

<p>Let‚Äôs summarize the results. Open LLMs are:</p>

<ol>
  <li>more private than closed LLM adaptations since they have significantly fewer possibilities for privacy leakage;</li>
  <li>more performant than closed LLM adaptations: at the same privacy level, even using much smaller models, we can obtain higher performance with open LLMs due to their ability to support gradient-based adaptation methods;</li>
  <li>more cost-effective than closed LLM adaptations that incur continuous query costs to an LLM provider.</li>
</ol>

<p>If you‚Äôd like to learn more, read our NeurIPS 2024 paper <a href="https://adam-dziedzic.com/static/assets/papers/openLLMs.pdf">‚ÄúOpen LLMs are Necessary for Current Private Adaptations and Outperform their Closed Alternatives‚Äù</a>. It goes into greater detail about our research on this topic.</p>]]></content><author><name></name></author><category term="private" /><category term="prompts" /><category term="LLMs" /><category term="DPSGD" /><category term="PATE" /><category term="OpenLLMs" /><category term="ClosedLLMs" /><category term="adaptations" /><summary type="html"><![CDATA[by Adam Dziedzic and Franziska Boenisch]]></summary></entry><entry><title type="html">How to prompt LLMs with private data?</title><link href="/2024/04/28/private-prompts.html" rel="alternate" type="text/html" title="How to prompt LLMs with private data?" /><published>2024-04-28T00:00:00+02:00</published><updated>2024-04-28T00:00:00+02:00</updated><id>/2024/04/28/private-prompts</id><content type="html" xml:base="/2024/04/28/private-prompts.html"><![CDATA[<p><em>by Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch</em></p>

<p>Over the past few years, large language models (LLMs) have gained widespread attention in both the tech industry and academia, as well as the public at large. LLMs are capable of executing a wide range of language-related tasks, such as translation, text generation, and sentiment analysis. One particularly amazing property of LLMs is that they need only minor modifications to handle new tasks, making them particularly well-suited for the rapid development of new applications.</p>

<p>In many scenarios, a company might want to adapt these pretrained LLMs with data that contains sensitive information. For example, Epic, a healthcare software company in the U.S., is recently partnering with Microsoft to integrate GPT4 in managing patients‚Äô electronic healthcare records. Adapting LLMs to this data naively could expose patients‚Äô sensitive information. To prevent these privacy violations, we have to be careful how we construct the prompts we query the LLM with - as we will explain next.</p>

<h3 id="what-is-prompting-for-llms">What is ‚Äòprompting‚Äô for LLMs?</h3>

<p>Generally speaking, there are two standard ways to adapt LLMs to new tasks, ‚Äúfine-tuning‚Äù and ‚Äúprompting.‚Äù The first way, fine-tuning, updates the parameters of the LLM to better reflect the new task. The second, prompting, does not make any updates to the original LLMs. Instead, it adds examples to provide context for the input that the user submits. This is done directly when the model is asked to predict on an input from the new task.</p>

<p>A canonical type of a prompt contains a brief instruction of the task, followed by several examples of the task in the form of an input and output pair. For example, an LLM could be used to recognize whether the sentiment of a movie review was positive or negative using a prompt that looks like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Instruction: Given some text, classify whether it is positive or negative.
Input: the movie was great.
Output: positive
</code></pre></div></div>

<p>To get the model‚Äôs prediction on a new movie review, we would prepend the above prompt to the new query ‚ÄúInput: the film was rather boring.‚Äù The model would then output the following: ‚ÄúOutput: negative‚Äù.</p>

<h3 id="why-prompting">Why prompting?</h3>

<p>Prompting offers several advantages over fine-tuning. First, prompting is very storage-efficient. Storing a separate fine-tuned model for each new task can be expensive because LLMs have lots of parameters. Instead, prompt tuning only requires storing a small prompt to provide task-specific context.</p>

<p>Second, the computation costs of fine-tuning become increasingly prohibitive as the models‚Äô sizes grow. Prompting, as stated, works without tuning all of the model parameters.</p>

<p>Third, many LLMs are deployed behind APIs‚Äìa restricted interface for two or more computer programs to communicate with each other. This means that the end user does not have access to the model parameters. The only way to adapt the model in this case is through prompts.</p>

<h3 id="the-privacy-risks-of-prompting">The privacy risks of prompting</h3>

<p>When we started our work, little was known about the privacy implications of prompting. The first question we asked ourselves is: do LLM predictions leak sensitive information contained in their prompts? To measure this, we carried out what is called a ‚Äúmembership inference attack‚Äù on model predictions. Given a particular example, this attack tries to figure out if the LLM used that example in its prompt when making predictions. This may be a very simple attack - but it can be used to construct much more sophisticated attacks like attacks that reconstruct the examples contained in prompts. Here are the results from this first experiment.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/prompts/miaprompt.png" width="90%" /></p>
</div>

<p>The left figure shows that the LLM is much more confident when being asked to predict on an input which it already appears in the prompt (as indicated in blue) than when it is asked to predict on an input that was not in the prompt (as indicated in orange). This means the adversary can ‚Äúguess‚Äù that an example was in the prompt by looking at the examples for which the model is confident were in the prompt. The right figure shows that this guess is often a correct one. The ROC curve (receiver operating characteristic curve) of this attack demonstrates the adversary (indicated with the blue curve) is more likely to succeed than if it had guessed randomly (using a coin flip, as indicated by the red dotted line). Indeed, the further away the blue line is from the red line, the more successful the attack is.</p>

<p>Put altogether, this first experiment shows that there is significant risk when one includes sensitive data in the prompts that are given to LLMs. So, we set out to find a solution.</p>

<h3 id="privacy-preserving-prompts">Privacy-preserving prompts</h3>

<p>To protect privacy, intuitively, we would like to ensure that none of the examples contained in the prompt ‚Äúinfluences‚Äù the outputs of the prompt too much. This way, when the LLM makes predictions from the prompt, its output won‚Äôt reveal too much information about the particular examples that were used to construct the prompt. It turns out that the research community has developed a framework to capture this intuition more rigorously - this framework is called differential privacy (DP). If you are interested in more details on differential privacy, you are welcome to check out <a href="https://www.cleverhans.io/2021/01/14/privacy-focus-algorithms.html">our earlier blog post on the topic</a>.</p>

<p>How can we construct prompts with differential privacy? First, we prompt the LLM with different prompts, each of which contains a disjoint subset of examples from the private dataset. Given an (unlabeled) input, each prompted LLM outputs a prediction based on its own prompt. Then we gather all the predictions from each prompted model and output the prediction that most models agree with. Intuitively, this ‚Äúvoting‚Äù process makes a prediction based on the information contained in multiple data points and reduces the influence that each data point has on the overall prediction made.</p>

<p>The above process alone is almost but not quite sufficient to provide differential privacy guarantees. To rigorously protect the privacy of the examples contained in the private set, we have to add noise to the models‚Äô votes before we output the consensus. This approach is indeed well-established in the privacy-preserving machine learning literature. It is called Private Aggregation of Teacher Ensembles (PATE). For a detailed introduction to PATE, we refer you to <a href="https://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html">our another blog post</a>.</p>

<p>We then select the best pair of input and aggregated label and use that pair as the example that makes up our prompt for the LLM model. We refer to this prompt as the student prompt because this prompt was obtained by distilling the knowledge the teachers had acquired.</p>

<p>The diagram below illustrates this approach.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/prompts/promptpate.png" width="90%" /></p>
</div>

<p>We implement this PATE algorithm with two popular commercial LLM APIs, GPT3 and Claude. Our results show that the method offers very strong privacy protection for data contained in prompts as well as high prediction accuracy when using our private prompts. Our method is the first privacy-preserving algorithm that can be employed with all commercial APIs.</p>

<p>A subset of commercial APIs may provide additional access to the user. For instance, they may allow the user to compute gradients with respect to the inputs of the LLM being queried. When we have such access to the model, we can deploy another approach to obtain prompts that protect privacy.</p>

<p>These prompts are different to the ones we described until now. So far, we used what is called discrete prompts: the prompts were made up of real words. Yet, LLMs internally represent words as vectors of numbers. This can be exploited to prompt them more precisely. We call this type of prompts soft prompts, the LLM is prompted with vectors of numbers directly (rather than real words that are then later converted into vectors of numbers).</p>

<p>Since soft prompts are made of numbers, we can construct them using a search procedure that is based on gradient descent. Luckily, gradient descent is the most common way to train neural networks. Hence, the privacy community has devised numerous algorithms to make gradient descent differentially private. In particular, one algorithm stands out: differentially private stochastic gradient descent. It clips and noises gradients to make them private. Why do these extra steps protect the privacy of training data? Intuitively speaking, clipping the gradients ensures that an example cannot influence the model update too much, and adding noise obfuscates the particular updates that were applied to the model. Thus, by modifying the gradient descent procedure in training the soft prompts, the data used to train the soft prompts has a differential privacy guarantee.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/prompts/prmptdpsgd.png" width="50%" /></p>
</div>

<h3 id="conclusions">Conclusions</h3>

<p>Our work identifies the privacy risk of prompting and offers methods to construct discrete and soft prompts that preserve privacy. We find that prompting is a practical and efficient way to adapt LLMs with differential privacy. There are still many improvements that can be made to our algorithm. For example, it is interesting to see how to extend our approach to protect users against malicious LLM providers. We hope that our work will inspire others to work on this problem.</p>

<p>If you‚Äôd like to learn more, read our NeurIPS 2023 paper <a href="https://openreview.net/forum?id=u6Xv3FuF8N">‚ÄúFlocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models‚Äù</a>. It goes into greater detail about our research on this topic.</p>]]></content><author><name></name></author><category term="private" /><category term="prompts" /><category term="LLMs" /><category term="DPSGD" /><category term="PATE" /><summary type="html"><![CDATA[by Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch]]></summary></entry><entry><title type="html">Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders</title><link href="/2023/12/10/b4b.html" rel="alternate" type="text/html" title="Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders" /><published>2023-12-10T00:00:00+01:00</published><updated>2023-12-10T00:00:00+01:00</updated><id>/2023/12/10/b4b</id><content type="html" xml:base="/2023/12/10/b4b.html"><![CDATA[<center>
  <a href="https://scholar.google.com/citations?user=iG319iwAAAAJ&amp;hl=pl&amp;oi=ao">Jan Dubi≈Ñski*<sup>2,3</sup></a>,
  <a href="https://cvlab.ii.pw.edu.pl/author/stanis%C5%82aw-pawlak/">Stanis≈Çaw Pawlak*<sup>2</sup></a>,
  <a href="https://franziska-boenisch.de/">Franziska Boenisch*<sup>1</sup></a>,
  <a href="https://cvlab.ii.pw.edu.pl/ttrzcins/">Tomasz Trzci≈Ñski<sup>2,3,4</sup></a>,
  <a href="https://adam-dziedzic.com/">Adam Dziedzic<sup>1</sup></a>
  <br /><br />
  <sup>1</sup>CISPA Helmholtz Center for Information Security<br />
  <sup>2</sup>Warsaw University of Technology<br />
  <sup>3</sup>IDEAS NCBR<br />
  <sup>4</sup>Tooploox<br />
  <em>*Indicates Equal Contribution</em>
    <br /><br />
</center>

<h2 id="the-growing-risk-of-model-stealing"><strong>The Growing Risk of Model Stealing</strong></h2>

<p>Machine learning models are increasingly exposed via public APIs, making them susceptible to model stealing. Attackers can systematically query these APIs, gathering enough data to reconstruct a nearly identical model at a fraction of the cost. This poses serious security risks, as stolen models can be used for unfair competition, academic plagiarism, or even malicious purposes.</p>

<h2 id="understanding-encoders-why-are-they-easy-targets"><strong>Understanding Encoders: Why Are They Easy Targets?</strong></h2>

<p>Encoders are a crucial component of modern AI, leveraging <strong>Self-Supervised Learning (SSL)</strong>. Unlike classification models that output simple labels, encoders generate <strong>high-dimensional feature representations</strong> that capture complex data patterns.</p>

<p>This flexibility also makes them highly <strong>vulnerable to theft</strong>:</p>

<ul>
  <li>Their <strong>rich outputs leak more information</strong> compared to supervised models.</li>
  <li>They require <strong>expensive training</strong> but can be cheaply stolen with API queries.</li>
  <li>Attackers can efficiently <strong>replicate their functionality</strong> without needing the original training data.</li>
</ul>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/sl_vs_ssl.png" width="50%" /></p>
</div>

<h2 id="how-model-stealing-works"><strong>How Model Stealing Works</strong></h2>

<p>Model stealing attacks exploit the public accessibility of machine learning models by systematically querying them and using the returned outputs to train a substitute model. Attackers typically follow these steps:</p>

<ol>
  <li><strong>Querying the Model</strong>: The attacker sends numerous carefully crafted inputs to the target model via its API.</li>
  <li><strong>Collecting Responses</strong>: Unlike classifiers that return class labels, encoders return rich feature representations, which reveal much more information about the model behaviour.</li>
  <li><strong>Training a Stolen Model</strong>: The attacker trains a new model using the collected representations, similarly as in the knowledge distillation process.</li>
</ol>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/stealing_ssl.png" width="50%" /></p>
</div>

<p>Because encoders output high-dimensional embeddings, they provide significantly more information per query than classifiers, making them even more vulnerable to extraction attacks.</p>

<h2 id="key-observation-how-attackers-interact-with-encoders"><strong>Key Observation: How Attackers Interact with Encoders</strong></h2>

<p>We start from a fundamental observation that attackers and legitimate users interact with the encoder in fundamentally different ways. <strong>Legitimate users</strong> typically use the encoder to solve a <strong>particular downstream task</strong>, meaning they operate within a limited and task-specific region of the embedding space. In contrast, <strong>attackers trying to steal the model must explore much larger portions of the encoder‚Äôs latent space</strong> to reconstruct its full functionality.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/legit_coverage.png" width="50%" /></p>
</div>
<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/attacker_coverage.png" width="50%" /></p>
</div>

<p>This discrepancy enables an <strong>active defense mechanism</strong>: by tracking how much of the embedding space a user explores, we can distinguish between normal usage and adversarial behavior. Attackers will necessarily have queries that span a broader distribution, covering significantly larger portions of the embedding space compared to legitimate users who remain within a specific subspace.</p>

<h2 id="measuring-latent-space-coverage-to-prevent-theft">Measuring Latent Space Coverage to Prevent Theft</h2>

<p>Building on our key observation, we introduce a method to quantify how much of the latent space a user explores. Instead of treating all queries equally, we <strong>divide the latent space into buckets</strong> and <strong>measure how unique buckets a user‚Äôs queries occupy</strong>.</p>

<p>To achieve this, we leverage <strong>Locality-Sensitive Hashing (LSH)</strong>, a technique designed to map high-dimensional embeddings into discrete hash buckets. LSH enables us to track how broadly a user‚Äôs queries are distributed across the latent space.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/LSH.png" width="50%" /></p>
</div>

<p>We associate high latent space (buckets) coverage with an extremely high query cost and low coverage with a negligible cost.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/buckets.png" width="50%" /></p>
</div>

<h2 id="b4b-a-way-to-prevent-model-theft"><strong>B4B: A Way to Prevent Model Theft</strong></h2>

<p>Now, we can describe our <strong>Bucks for Buckets</strong> end-to-end! B4B is designed to stop attackers <strong>as they attempt</strong> to steal encoders, ensuring that regular users receive high-quality outputs while making theft prohibitively expensive. It achieves this through three key mechanisms:</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/simple_overview.png" width="50%" /></p>
</div>

<ol>
  <li><strong>Tracking Query Behavior</strong> - B4B estimates how much of the encoder‚Äôs embedding space a user is exploring using <strong>Locality-Sensitive Hashing (LSH)</strong>. Legitimate users, who focus on specific tasks, naturally stay within smaller regions, whereas adversaries trying to steal the model attempt to cover much larger areas.</li>
</ol>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/coverage.png" width="50%" /></p>
</div>

<ol>
  <li><strong>Dynamic Penalties</strong> - Based on the user‚Äôs query behavior, B4B gradually <strong>introduces noise</strong> to the encoder‚Äôs outputs, making it harder for attackers to reconstruct the stolen model. This penalty escalates as the adversary‚Äôs exploration of the embedding space increases.</li>
</ol>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/cost_function.png" width="50%" /></p>
</div>

<ol>
  <li><strong>Personalized Responses</strong> - Additionaly, to counter <strong>Sybil attacks</strong>, where an adversary creates multiple accounts to evade detection, B4B applies unique transformations to each user‚Äôs representations. This prevents attackers from merging extracted responses across accounts to reconstruct the encoder.</li>
</ol>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/transformations.png" width="50%" /></p>
</div>

<h2 id="b4b-in-action-stopping-model-theft-without-compromising-usability"><strong>B4B in Action: Stopping Model Theft Without Compromising Usability</strong></h2>

<p>We tested B4B on <strong>popular SSL models</strong>, including <strong>SimSiam</strong> and <strong>DINO</strong>, against cutting-edge stealing techniques. Our findings demonstrate that B4B:</p>

<ul>
  <li><strong>Reduces stolen model performance by up to 60%</strong>, while ensuring legitimate users experience less than a 1% drop in utility.</li>
  <li><strong>Effectively neutralizes Sybil attacks</strong>, making it infeasible to aggregate stolen data from multiple accounts.</li>
  <li><strong>Outperforms previous security methods</strong>, which either failed to prevent theft or significantly degraded output quality for all users.</li>
</ul>

<h2 id="enabling-more-secure-ai-apis"><strong>Enabling More Secure AI APIs</strong></h2>

<p>As AI companies increasingly offer <strong>pre-trained SSL models</strong> through public APIs, solutions like <strong>B4B</strong> will be essential in protecting intellectual property. Our modular framework allows for adaptation to various security policies and evolving threats.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/b4b/overview.png" width="50%" /></p>
</div>

<hr />

<p>For further details, check out
<a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/ad1efab57a04d93f097e7fbb2d4fc054-Abstract-Conference.html" target="_blank">our full research paper presented at NeurIPS 2023.</a></p>

<h2 id="bibtex">Bibtex</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{dubinski2023bucks,
  title = {Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders},
  author = {Dubi≈Ñski, Jan and Pawlak, Stanis≈Çaw and Boenisch, Franziska and Trzcinski, Tomasz and Dziedzic, Adam},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2023}
}
</code></pre></div></div>]]></content><author><name></name></author><category term="model" /><category term="extraction," /><category term="model" /><category term="stealing," /><category term="model" /><category term="functionality" /><category term="stealing," /><category term="self-supervised" /><category term="learning," /><category term="deep" /><category term="learning," /><category term="b4b," /><category term="stealing" /><category term="encoders" /><summary type="html"><![CDATA[Jan Dubi≈Ñski*2,3, Stanis≈Çaw Pawlak*2, Franziska Boenisch*1, Tomasz Trzci≈Ñski2,3,4, Adam Dziedzic1 1CISPA Helmholtz Center for Information Security 2Warsaw University of Technology 3IDEAS NCBR 4Tooploox *Indicates Equal Contribution]]></summary></entry><entry><title type="html">On stealing and defending self-supervised models</title><link href="/2023/02/23/model-extraction.html" rel="alternate" type="text/html" title="On stealing and defending self-supervised models" /><published>2023-02-23T00:00:00+01:00</published><updated>2023-02-23T00:00:00+01:00</updated><id>/2023/02/23/model-extraction</id><content type="html" xml:base="/2023/02/23/model-extraction.html"><![CDATA[<p><em>by Adam Dziedzic and Nicolas Papernot</em></p>

<p><em>This blog post is part of a series on model stealing, check out our previous blog posts
<a href="http://www.cleverhans.io/2021/04/28/is-this-model-mine.html">‚ÄúIs this model mine?‚Äù</a>
for a general introduction to the problem and a new active defense against the extraction of supervised models <a href="http://www.cleverhans.io/2022/04/21/pow-defense.html">
‚ÄúHow to Keep a Model Stealing Adversary Busy?‚Äù</a>
.<a href="https://openreview.net/forum?id=EAy7C1cgE1L">[1]</a></em></p>

<p>Machine Learning (ML) models cater to tasks such as language translation, speech recognition, data annotation, or
optical character recognition.
The models that fulfill these tasks are publicly exposed via APIs. They are trained in either the
supervised or self-supervised setting. The supervised models are expensive in terms of data labeling but their training
cost is relatively inexpensive. On the other hand, the self-supervised models have zero cost of data labeling. They can
leverage all the available data points but such an approach increases the training cost of these models. The creators of
the models want to prevent
them from being stolen. In this blog post, we analyze how to steal and defend self-supervised
models <a href="https://arxiv.org/abs/2205.07890">[2]</a>
, <a href="https://arxiv.org/abs/2209.09024">[3]</a>.
The threat of stealing self-supervised learning models is real. An attacker‚Äôs
incentive is to steal a model at a much lower cost than training it from scratch. Recently, researchers showed that
training a ResNet50
SSL model costs north of $5713.92, whereas stealing such an encoder costs only around
$72.49 <a href="https://arxiv.org/abs/2201.11692">[4]</a>.</p>

<h3 id="self-supervised-learning">Self-Supervised Learning</h3>

<p>Self-supervised learning (SSL) emerges as a dominant learning paradigm behind modern ML APIs. The major paradigm shift
is that
these large self-supervised models, called encoders, are trained on a big amount of unlabeled data. These SSL models
return high-dimensional representations instead of low-dimensional outputs such as labels. The representations are
features extracted from a given input query and they can be used for many downstream tasks. For example, you can send a
block of text as input and receive an embedding as output, which is offered by an API exposed by
Cohere <a href="https://txt.cohere.ai/sentence-word-embeddings/">[5]</a>.
Going from supervised to self-supervised learning is essential and it is the future of the ML APIs. It is essential
since representations can be re-used on many tasks and you need a small number of labels to train downstream tasks.
A company that offers such an API does not have to know all of the labels required by a given client but they provide
a generic interface that extracts useful features for a given input.
First, we will show how to extract encoders and then present methods to detect if a given encoder is a stolen copy of a
victim encoder.</p>

<h3 id="how-to-steal-self-supervised-encoders">How to Steal Self-Supervised Encoders?</h3>

<p>The framework of our attacks is inspired by Siamese networks. For an input query, represented as an image of a (corgi)
dog (in the diagram below), we generate two augmentations by applying vertical flip and grayscale transformation.
Then, we query the victim encoder with the first augmentation and our stolen copy with the second augmentation. To steal
the victim encoder, we train the stolen encoder to minimize the loss between representations obtained from the victim
encoder \(y_1\) shown in blue, and representations from our stolen copy \(ùë¶_2\) shown in red.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/ssl/siamese.png" width="90%" /></p>
</div>

<p>To this end, we apply contrastive loss functions (see the image below). In the representation space, before stealing,
the positive pairs (two
augmentations of the same image \(y_1^+\) and \(y_2^+\)) from
the victim and stolen encoders are far
from each other. After stealing, the positive pairs are close to each other. Thus, the stolen encoder replicates the
behavior of the victim encoder. The crucial point of contrastive loss functions is that the positive pairs stay close to
each other but also the negative examples (\(y_1^+\) and \(y_3^-\)) coming from different inputs are far from the
positive pairs.
The selection of the loss function is one of the most important hyper-parameter for stealing encoders. We compared
standard losses as well as modern batch contrastive losses. The standard loss like Mean Squared Error is used to
directly minimize distances between representations from the victim and the stolen copy. Modern batch contrastive losses
like InfoNCE <a href="https://arxiv.org/abs/2002.05709">[8]</a> or Soft Nearest
Neighbors <a href="https://proceedings.mlr.press/v97/frosst19a.html">[9]</a> compare not only positive but also negative pair
samples. We find that
contrastive losses
perform the best for both training and stealing encoders and allow us to decrease the number of stealing queries to be
less than 1/5th of the number of victim‚Äôs training data points.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/ssl/contrastive_loss.png" width="90%" /></p>
</div>

<h3 id="how-to-defend-or-detect-encoder-stealing">How to Defend or Detect Encoder Stealing?</h3>

<p>Having discussed how to steal encoders, let us consider defense methods. The active defenses either perturb or truncate
the answers to poison the training objective of an attacker but they were shown to harm substantially the performance of
legitimate users so they are not usable in the SSL setting <a href="https://arxiv.org/abs/2201.05889">[6]</a>. The watermarking
based defenses, for example, embed a unique task into the encoder, which marks the encoder as our property. If we can
prove that the stolen encoder also has our unique property, then we can detect a theft. During standard training of
encoders, we provide as inputs images and train the encoder to generate high-quality representations. For watermarking,
we embed the rotation task into encoders during training. As an example, the task is a binary classification where we
have to classify if a given image is rotated between 0 and 180 degrees or between 180 and 360 degrees. To implement the
watermark, we add the additional fully-connected layer on top of the representations. Whenever we tune the parameters
for the
rotation task, we also adapt all the other weights in the encoder. As a result, the watermarked encoder and its stolen
copies return not only embeddings but also the correct rotation range (see the schema below). We observe that for a
legitimately trained model, the watermark task achieves near-random accuracy of 50%. On
the other hand, for a stolen encoder, the more queries are used for extraction, the higher transfer of the watermark (&gt;
50%).
This holds across many loss functions used for stealing.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/ssl/watermark.png" width="90%" /></p>
</div>

<p>However, first, watermarking requires retraining or fine-tuning the large encoders, which is impractical. An adaptive
attacker can use different extraction methods to lower the transfer of the watermark from the victim to stolen encoders.
There exist many methods to remove watermarks. For example, in the case of the rotation watermark, an adversary can
obfuscate the representations to fool the detection of a watermark. To tackle the problems of watermarking, we propose a
new method to detect a stolen encoder. Our approach is based on dataset inference that treats the training data of a
victim encoder as its signature <a href="https://openreview.net/forum?id=hvdKKV2yt7T">[7]</a>. This detection method is effective
and does not require encoder fine-tuning.</p>

<p>For the resolution of the ownership, we assume that we as a defender have access to our training and test sets as well
as the query access to a suspect encoder. Training and test data come from the same distribution. The first step in our
method
is to train a
meta model, in this case a Gaussian Mixture Model (GMM) to estimate the distribution of representations. We use part of
the
traing set to do so. To resolve the ownership, we compare the log-likelihoods for the train vs test sets. In the case
when
the log-likelihood is much larger for the train set, we mark the tested encoder as stolen. Otherwise, if there is almost
no difference in the log-likelihood between the train and test sets, then such an encoder is marked as an independent
encoder. How do we quantify if the difference between the log-likelihoods of representations are significant? We do it
by harnessing statistical testing. Concretely, to verify
the ownership we use a statistical t-test. The null hypothesis is that there is no difference between the log-likelihood
for train and test sets. If the p-value is below a certain threshold (e.g., 1%,) then the encoder is
either the victim or stolen. Otherwise, the t-test is inconclusive and the suspect encoder is marked as independent,
meaning not stolen.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/ssl/dataset_inference.png" width="90%" /></p>
</div>

<h3 id="summary">Summary</h3>

<p>Let us summarize the main aspects stealing attacks and defenses for SSL models. Recent attacks on the SSL models show
that
high-quality encoders can be extracted at the fraction of the cost of creating the victim encoders.
We described how we can detect stolen self-supervised models by using the victim‚Äôs training and test data as the model‚Äôs
signature. The crucial intuition is that an encoder that was trained on the given training data or stolen from such
victim behaves differently on the training data vs the test data. On the other hand, an independently trained encoder
behaves similarly on both the train and test data. As of now, we can only detect stolen encoders. Defending encoders
against stealing attacks is challenging since representations leak large amounts of information and the existing
defenses cannot be applied out-of-the-box to protect encoders. Thus, there is a need to create active defenses for
self-supervised models which would not harm legitimate users but could increase the cost of encoder extraction.</p>

<h3 id="want-to-read-more">Want to read more?</h3>

<p>You can find additional details in our <a href="https://arxiv.org/abs/2205.07890">ICML paper</a> [2] and
the <a href="https://arxiv.org/abs/2209.09024">NeurIPS paper</a> [3]. The code for reproducing all our experiments is available in
our GitHub repositories: <a href="https://github.com/cleverhans-lab/ssl-attacks-defenses">SSLAttackDefenses</a>
and <a href="https://github.com/cleverhans-lab/DatasetInferenceForSelfSupervisedModels">DataSetInferenceForSelfSupervisedModels</a>
.</p>

<hr />
<p><em>[1] Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, Nicolas
Papernot. <strong><a href="https://openreview.net/forum?id=EAy7C1cgE1L">Increasing the Cost of Model Extraction with Calibrated Proof of Work</a></strong>
ICLR 2022.</em></p>

<p><em>[2] Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad Kaleem, Jonas Guan, Nicolas
Papernot. <strong><a href="https://arxiv.org/abs/2205.07890">On the Difficulty of Defending Self-Supervised Learning against Model Extraction</a></strong>
ICML 2022.</em></p>

<p><em>[3] Adam Dziedzic, Haonan Duan, Muhammad Ahmad Kaleem, Nikita Dhawan, Jonas Guan, Yannis Cattan, Franziska Boenisch,
Nicolas Papernot.
<strong><a href="https://arxiv.org/abs/2209.09024">Dataset Inference for Self-Supervised Models</a></strong> NeurIPS 2022.</em></p>

<p><em>[4] Tianshuo Cong, Xinlei He, Yang
Zhang. <strong><a href="https://arxiv.org/abs/2201.11692">SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders</a></strong>
CCS 2022.</em></p>

<p><em>[5] Luis Serrano. <strong><a href="https://txt.cohere.ai/sentence-word-embeddings/">What Are Word and Sentence Embeddings</a></strong>?</em></p>

<p><em>[6] Yupei Liu, Jinyuan Jia, Hongbin Liu, Neil Zhenqiang
Gong. <strong><a href="https://arxiv.org/abs/2201.05889">StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning</a></strong>
CCS 2022.</em></p>

<p><em>[7] Pratyush Maini, Mohammad Yaghini, Nicolas Papernot.
<strong><a href="https://openreview.net/forum?id=hvdKKV2yt7T">Dataset Inference: Ownership Resolution in Machine Learning</a></strong> ICLR
2021.</em></p>

<p><em>[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey
Hinton. <strong><a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a></strong>
ICML 2020.</em></p>

<p><em>[9] Nicholas Frosst, Nicolas Papernot, Geoffrey
Hinton. <strong><a href="https://proceedings.mlr.press/v97/frosst19a.html">Analyzing and Improving Representations with the Soft Nearest Neighbor Loss</a></strong>
ICML 2020.</em></p>]]></content><author><name></name></author><category term="model" /><category term="extraction," /><category term="model" /><category term="stealing," /><category term="model" /><category term="functionality" /><category term="stealing," /><category term="self-supervised" /><category term="learning," /><category term="deep" /><category term="learning" /><summary type="html"><![CDATA[by Adam Dziedzic and Nicolas Papernot This blog post is part of a series on model stealing, check out our previous blog posts ‚ÄúIs this model mine?‚Äù for a general introduction to the problem and a new active defense against the extraction of supervised models ‚ÄúHow to Keep a Model Stealing Adversary Busy?‚Äù .[1] Machine Learning (ML) models cater to tasks such as language translation, speech recognition, data annotation, or optical character recognition. The models that fulfill these tasks are publicly exposed via APIs. They are trained in either the supervised or self-supervised setting. The supervised models are expensive in terms of data labeling but their training cost is relatively inexpensive. On the other hand, the self-supervised models have zero cost of data labeling. They can leverage all the available data points but such an approach increases the training cost of these models. The creators of the models want to prevent them from being stolen. In this blog post, we analyze how to steal and defend self-supervised models [2] , [3]. The threat of stealing self-supervised learning models is real. An attacker‚Äôs incentive is to steal a model at a much lower cost than training it from scratch. Recently, researchers showed that training a ResNet50 SSL model costs north of $5713.92, whereas stealing such an encoder costs only around $72.49 [4]. Self-Supervised Learning Self-supervised learning (SSL) emerges as a dominant learning paradigm behind modern ML APIs. The major paradigm shift is that these large self-supervised models, called encoders, are trained on a big amount of unlabeled data. These SSL models return high-dimensional representations instead of low-dimensional outputs such as labels. The representations are features extracted from a given input query and they can be used for many downstream tasks. For example, you can send a block of text as input and receive an embedding as output, which is offered by an API exposed by Cohere [5]. Going from supervised to self-supervised learning is essential and it is the future of the ML APIs. It is essential since representations can be re-used on many tasks and you need a small number of labels to train downstream tasks. A company that offers such an API does not have to know all of the labels required by a given client but they provide a generic interface that extracts useful features for a given input. First, we will show how to extract encoders and then present methods to detect if a given encoder is a stolen copy of a victim encoder. How to Steal Self-Supervised Encoders? The framework of our attacks is inspired by Siamese networks. For an input query, represented as an image of a (corgi) dog (in the diagram below), we generate two augmentations by applying vertical flip and grayscale transformation. Then, we query the victim encoder with the first augmentation and our stolen copy with the second augmentation. To steal the victim encoder, we train the stolen encoder to minimize the loss between representations obtained from the victim encoder \(y_1\) shown in blue, and representations from our stolen copy \(ùë¶_2\) shown in red. To this end, we apply contrastive loss functions (see the image below). In the representation space, before stealing, the positive pairs (two augmentations of the same image \(y_1^+\) and \(y_2^+\)) from the victim and stolen encoders are far from each other. After stealing, the positive pairs are close to each other. Thus, the stolen encoder replicates the behavior of the victim encoder. The crucial point of contrastive loss functions is that the positive pairs stay close to each other but also the negative examples (\(y_1^+\) and \(y_3^-\)) coming from different inputs are far from the positive pairs. The selection of the loss function is one of the most important hyper-parameter for stealing encoders. We compared standard losses as well as modern batch contrastive losses. The standard loss like Mean Squared Error is used to directly minimize distances between representations from the victim and the stolen copy. Modern batch contrastive losses like InfoNCE [8] or Soft Nearest Neighbors [9] compare not only positive but also negative pair samples. We find that contrastive losses perform the best for both training and stealing encoders and allow us to decrease the number of stealing queries to be less than 1/5th of the number of victim‚Äôs training data points. How to Defend or Detect Encoder Stealing? Having discussed how to steal encoders, let us consider defense methods. The active defenses either perturb or truncate the answers to poison the training objective of an attacker but they were shown to harm substantially the performance of legitimate users so they are not usable in the SSL setting [6]. The watermarking based defenses, for example, embed a unique task into the encoder, which marks the encoder as our property. If we can prove that the stolen encoder also has our unique property, then we can detect a theft. During standard training of encoders, we provide as inputs images and train the encoder to generate high-quality representations. For watermarking, we embed the rotation task into encoders during training. As an example, the task is a binary classification where we have to classify if a given image is rotated between 0 and 180 degrees or between 180 and 360 degrees. To implement the watermark, we add the additional fully-connected layer on top of the representations. Whenever we tune the parameters for the rotation task, we also adapt all the other weights in the encoder. As a result, the watermarked encoder and its stolen copies return not only embeddings but also the correct rotation range (see the schema below). We observe that for a legitimately trained model, the watermark task achieves near-random accuracy of 50%. On the other hand, for a stolen encoder, the more queries are used for extraction, the higher transfer of the watermark (&gt; 50%). This holds across many loss functions used for stealing. However, first, watermarking requires retraining or fine-tuning the large encoders, which is impractical. An adaptive attacker can use different extraction methods to lower the transfer of the watermark from the victim to stolen encoders. There exist many methods to remove watermarks. For example, in the case of the rotation watermark, an adversary can obfuscate the representations to fool the detection of a watermark. To tackle the problems of watermarking, we propose a new method to detect a stolen encoder. Our approach is based on dataset inference that treats the training data of a victim encoder as its signature [7]. This detection method is effective and does not require encoder fine-tuning. For the resolution of the ownership, we assume that we as a defender have access to our training and test sets as well as the query access to a suspect encoder. Training and test data come from the same distribution. The first step in our method is to train a meta model, in this case a Gaussian Mixture Model (GMM) to estimate the distribution of representations. We use part of the traing set to do so. To resolve the ownership, we compare the log-likelihoods for the train vs test sets. In the case when the log-likelihood is much larger for the train set, we mark the tested encoder as stolen. Otherwise, if there is almost no difference in the log-likelihood between the train and test sets, then such an encoder is marked as an independent encoder. How do we quantify if the difference between the log-likelihoods of representations are significant? We do it by harnessing statistical testing. Concretely, to verify the ownership we use a statistical t-test. The null hypothesis is that there is no difference between the log-likelihood for train and test sets. If the p-value is below a certain threshold (e.g., 1%,) then the encoder is either the victim or stolen. Otherwise, the t-test is inconclusive and the suspect encoder is marked as independent, meaning not stolen. Summary Let us summarize the main aspects stealing attacks and defenses for SSL models. Recent attacks on the SSL models show that high-quality encoders can be extracted at the fraction of the cost of creating the victim encoders. We described how we can detect stolen self-supervised models by using the victim‚Äôs training and test data as the model‚Äôs signature. The crucial intuition is that an encoder that was trained on the given training data or stolen from such victim behaves differently on the training data vs the test data. On the other hand, an independently trained encoder behaves similarly on both the train and test data. As of now, we can only detect stolen encoders. Defending encoders against stealing attacks is challenging since representations leak large amounts of information and the existing defenses cannot be applied out-of-the-box to protect encoders. Thus, there is a need to create active defenses for self-supervised models which would not harm legitimate users but could increase the cost of encoder extraction. Want to read more? You can find additional details in our ICML paper [2] and the NeurIPS paper [3]. The code for reproducing all our experiments is available in our GitHub repositories: SSLAttackDefenses and DataSetInferenceForSelfSupervisedModels . [1] Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, Nicolas Papernot. Increasing the Cost of Model Extraction with Calibrated Proof of Work ICLR 2022. [2] Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad Kaleem, Jonas Guan, Nicolas Papernot. On the Difficulty of Defending Self-Supervised Learning against Model Extraction ICML 2022. [3] Adam Dziedzic, Haonan Duan, Muhammad Ahmad Kaleem, Nikita Dhawan, Jonas Guan, Yannis Cattan, Franziska Boenisch, Nicolas Papernot. Dataset Inference for Self-Supervised Models NeurIPS 2022. [4] Tianshuo Cong, Xinlei He, Yang Zhang. SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders CCS 2022. [5] Luis Serrano. What Are Word and Sentence Embeddings? [6] Yupei Liu, Jinyuan Jia, Hongbin Liu, Neil Zhenqiang Gong. StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning CCS 2022. [7] Pratyush Maini, Mohammad Yaghini, Nicolas Papernot. Dataset Inference: Ownership Resolution in Machine Learning ICLR 2021. [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton. A Simple Framework for Contrastive Learning of Visual Representations ICML 2020. [9] Nicholas Frosst, Nicolas Papernot, Geoffrey Hinton. Analyzing and Improving Representations with the Soft Nearest Neighbor Loss ICML 2020.]]></summary></entry></feed>